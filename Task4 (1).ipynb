{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 is a subset of an 80 million image collection, with 60,000 images in total. The training data consists of 50k images, while the test data contains 10k images. The CIFAR-10 dataset consists of 10 categories with each category containing 6000 images.\n",
    "\n",
    "\n",
    "Our focus in this notebook will be on PyTorch for building and training on the CIFAR10 Dataset. PyTorch simplifies the learning process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the libraires required to run the CIFAR10 pytorch program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import time\n",
    "import torchvision\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and download the dataset\n",
    "\n",
    "the dataset has 100 batches and each batch will have 60000/100 = 6000 images and the test batch will have total of 10 k images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshapedataset():\n",
    "    transform_train = transforms.Compose([transforms.ToTensor(), # comvert the image to tensor so that it can work with torch\n",
    "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) #Normalize all the images\n",
    "                               ])\n",
    "    transform = transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                               ])\n",
    "    #transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "   # batch_size = 128\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                                download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                                  shuffle=True, num_workers=2 , pin_memory=True)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                               download=True, transform=transform_train)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                                 shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    return trainset, testset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the device and set the parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_available(): True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set the device \n",
    "\n",
    "torch.cuda.is_available()\n",
    "print('torch.cuda.is_available():', torch.cuda.is_available())\n",
    "torch.cuda.current_device()\n",
    "torch.cuda.get_device_name()\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set parameters\n",
    "inputsize = 3072\n",
    "numofclasses =10\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Neural Network Model\n",
    "\n",
    "We have built a Neural network Model of 5 hidden layers. Since we have colour images our dataset will be divided into numeric values of RGB.The first layer is with input node [3072,1024,512,64,64,10]. Forward pass will use Rectified Activation unit(ReLu) which is easy to compute. To avoid activating all the nodes a transistion node is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create fully connected Network\n",
    "\n",
    "class NN(nn.Module): #inherit from nn.module#\n",
    "    \n",
    "    def __init__(self,inputsize,Layer1,Layer2,numofclasses,activation_functions):\n",
    "        super(NN,self).__init__()\n",
    "        self.fc1 = nn.Linear(inputsize,1024)# input layer and first layer\n",
    "       \n",
    "        self.fc2 =  nn.Linear(1024,512)#second layer and output layer#\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(512,64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(64,64)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.fc5 = nn.Linear(64,10)\n",
    "        self.activation_functions = activation_functions\n",
    "    def forward(self,xb):\n",
    "        out = xb.view(xb.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc5(out)\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the accuracy\n",
    "\n",
    "Accuracy is predicted by number of correct prediction out of total predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def checkaccuracy(loader, model,nameofdataset):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x , y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            #x= x.reshape(x.shape[0],-1)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1) #max of the second dimension,gives the index\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "            \n",
    "        print(f'Got{num_correct/num_samples} with accuracy {float(num_correct)/float(num_samples)*100}', nameofdataset)\n",
    "        \n",
    "    model.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "Dataloader takes full advantage of our custom dataset class that inherits Dataset from torch library\n",
    " \n",
    " We have used Adam optimizer with betas and weight decay value.\n",
    " \n",
    " we first check the data for training set and validate the accuracy with the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Got0.6381799578666687 with accuracy 63.818 Train\n",
      "Got0.5169000029563904 with accuracy 51.690000000000005 Test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialise Artifical Neural Network\n",
    "\n",
    "model = NN(inputsize = inputsize,Layer1 = 1024, Layer2 = 512 , numofclasses = numofclasses, activation_functions = activation_functions).to(device)\n",
    "#Loss and optimiser\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "betas=(0.9,0.995)\n",
    "weight_decay=5e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=betas,weight_decay=weight_decay)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum =0.9)\n",
    "trainloader , testloader = reshapedataset()\n",
    "train_loader = DataLoader(trainloader , batch_size= batch_size, shuffle = False )\n",
    "test_loader = DataLoader(testloader , batch_size= batch_size, shuffle = False )\n",
    "for epoch in range(num_epochs):\n",
    "    #check for each each batch loader from a tuple\n",
    "    train_losses = []\n",
    "    train_accuracy = []\n",
    "    for batch, (data, targets) in enumerate(train_loader):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        #print(data.shape)\n",
    "        #reshape to single vector , correct shape\n",
    "        #trainloader= DataLoader(train_subset, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "        data = data.reshape(data.shape[0],-1)\n",
    "        #print(data.shape)\n",
    "        scores = model(data)\n",
    "        \n",
    "        loss = loss_criteria(scores,targets)\n",
    "        #print(scores)\n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        #adam\n",
    "        optimizer.step()\n",
    "checkaccuracy(train_loader,model,\"Train\")\n",
    "checkaccuracy(test_loader,model,\"Test\")\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions - 1\n",
    "\n",
    "As we can see above the accuracy for CIFAR 10 dataset with Neural network for training dataset is 63 % and testing is 51%. there is a difference of 12%. We can infer that Neural network is not successful in its prediction of images\n",
    "\n",
    "Future to this we have tried with less Layers but still there is no much improvement in the accuracy.\n",
    "\n",
    "Firstly let's try to tune the dataset and run the model to check how it is effecting the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    " def reshapedataset():\n",
    "        transform_train = transforms.Compose([transforms.ToTensor(), # comvert the image to tensor so that it can work with torch\n",
    "                                      transforms.Normalize((0.4913997551666284, 0.48215855929893703, 0.4465309133731618),\n",
    "                                  (0.24703225141799082, 0.24348516474564, 0.26158783926049628))\n",
    "                                ])\n",
    "        transform = transforms.Compose([\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.4913997551666284, 0.48215855929893703, 0.4465309133731618),\n",
    "                                      (0.24703225141799082, 0.24348516474564, 0.26158783926049628))])\n",
    "\n",
    "\n",
    "        trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                                    download=True, transform=transform_train)\n",
    "        trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                                      shuffle=True, num_workers=2 , pin_memory=True)\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                                   download=True, transform=transform_train)\n",
    "        testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                                     shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "        return trainset, testset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ayAxP-WsEyzr",
    "outputId": "1b3a4559-27a8-456c-c3d9-2d559c6d7109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Got0.6262999773025513 with accuracy 62.629999999999995 Train\n",
      "Got0.5084999799728394 with accuracy 50.849999999999994 Test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialise Artifical Neural Network\n",
    "\n",
    "model = NN(inputsize = inputsize,Layer1 = 1024, Layer2 = 512 , numofclasses = numofclasses, activation_functions = activation_functions).to(device)\n",
    "#Loss and optimiser\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "betas=(0.9,0.995)\n",
    "weight_decay=5e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=betas,weight_decay=weight_decay)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum =0.9)\n",
    "trainloader , testloader = reshapedataset()\n",
    "train_loader = DataLoader(trainloader , batch_size= batch_size, shuffle = False )\n",
    "test_loader = DataLoader(testloader , batch_size= batch_size, shuffle = False )\n",
    "for epoch in range(num_epochs):\n",
    "    #check for each each batch loader from a tuple\n",
    "    train_losses = []\n",
    "    train_accuracy = []\n",
    "    for batch, (data, targets) in enumerate(train_loader):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        #print(data.shape)\n",
    "        #reshape to single vector , correct shape\n",
    "        #trainloader= DataLoader(train_subset, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "        data = data.reshape(data.shape[0],-1)\n",
    "        #print(data.shape)\n",
    "        scores = model(data)\n",
    "        \n",
    "        loss = loss_criteria(scores,targets)\n",
    "        #print(scores)\n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        #adam\n",
    "        optimizer.step()\n",
    "checkaccuracy(train_loader,model,\"Train\")\n",
    "checkaccuracy(test_loader,model,\"Test\")\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions -2 \n",
    "\n",
    "As we see still there is sigificant difference between Train and test accuracy and no much improvement lets add  dropout of p=0.2 in the Neural network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "rO5xurgkp_nr"
   },
   "outputs": [],
   "source": [
    "class NN_dropout(nn.Module): #inherit from nn.module#\n",
    "    \n",
    "    def __init__(self,inputsize,Layer1,Layer2,numofclasses,activation_functions,p):\n",
    "        super(NN_dropout,self).__init__()\n",
    "        #dropoutpercentage = 0.2\n",
    "\n",
    "        self.fc1 = nn.Linear(inputsize,1024)# input layer and first layer\n",
    "        self.dropout1 = nn.Dropout(p=p)\n",
    "        self.fc2 = nn.Linear(1024,512)#second layer and output layer#\n",
    "        self.dropout2 = nn.Dropout(p=p)\n",
    "        self.fc3 = nn.Linear(512,64)\n",
    "        self.dropout3 = nn.Dropout(p=p)\n",
    "        \n",
    "        self.fc4 = nn.Linear(64,64)\n",
    "        self.dropout4 = nn.Dropout(p=p)\n",
    "\n",
    "        self.fc5 = nn.Linear(64,10)\n",
    "        self.dropout5 = nn.Dropout(p=p)\n",
    "        \n",
    "    def forward(self,xb):\n",
    "        out = xb.view(xb.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc5(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Got0.6245799660682678 with accuracy 62.458000000000006 Train\n",
      "Got0.5072000026702881 with accuracy 50.72 Test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialise Artifical Neural Network\n",
    "\n",
    "modeldropout = NN_dropout(inputsize = inputsize,Layer1 = 1024, Layer2 = 512 , numofclasses = numofclasses, activation_functions = activation_functions,p=0.2).to(device)\n",
    "#Loss and optimiser\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "betas=(0.9,0.995)\n",
    "weight_decay=5e-4\n",
    "optimizer = optim.Adam(modeldropout.parameters(), lr=learning_rate, betas=betas,weight_decay=weight_decay)\n",
    "#optimizer = optim.SGD(modeldropout.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "\n",
    "trainloader , testloader = reshapedataset()\n",
    "train_loader = DataLoader(trainloader , batch_size= batch_size, shuffle = False )\n",
    "test_loader = DataLoader(testloader , batch_size= batch_size, shuffle = False )\n",
    "for epoch in range(num_epochs):\n",
    "    #check for each each batch loader from a tuple\n",
    "    \n",
    "    for batch, (data, targets) in enumerate(train_loader):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        #print(data.shape)\n",
    "        #reshape to single vector , correct shape\n",
    "        #trainloader= DataLoader(train_subset, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "        data = data.reshape(data.shape[0],-1)\n",
    "        #print(data.shape)\n",
    "        scores = modeldropout(data)\n",
    "        \n",
    "        loss = loss_criteria(scores,targets)\n",
    "        #print(scores)\n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        #adam\n",
    "        optimizer.step()\n",
    "            \n",
    "checkaccuracy(train_loader,modeldropout,\"Train\")\n",
    "checkaccuracy(test_loader,modeldropout,\"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Lets add a dropout of 0.5 and check how it is effecting the accuracy and make a conclusion on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Got0.6253600120544434 with accuracy 62.536 Train\n",
      "Got0.5185999870300293 with accuracy 51.85999999999999 Test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialise Artifical Neural Network\n",
    "\n",
    "modeldropout = NN_dropout(inputsize = inputsize,Layer1 = 1024, Layer2 = 512 , numofclasses = numofclasses, activation_functions = activation_functions,p=0.5).to(device)\n",
    "#Loss and optimiser\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "betas=(0.9,0.995)\n",
    "weight_decay=5e-4\n",
    "optimizer = optim.Adam(modeldropout.parameters(), lr=learning_rate, betas=betas,weight_decay=weight_decay)\n",
    "#optimizer = optim.SGD(modeldropout.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "\n",
    "trainloader , testloader = reshapedataset()\n",
    "train_loader = DataLoader(trainloader , batch_size= batch_size, shuffle = False )\n",
    "test_loader = DataLoader(testloader , batch_size= batch_size, shuffle = False )\n",
    "for epoch in range(num_epochs):\n",
    "    #check for each each batch loader from a tuple\n",
    "    \n",
    "    for batch, (data, targets) in enumerate(train_loader):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        #print(data.shape)\n",
    "        #reshape to single vector , correct shape\n",
    "        #trainloader= DataLoader(train_subset, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "        data = data.reshape(data.shape[0],-1)\n",
    "        #print(data.shape)\n",
    "        scores = modeldropout(data)\n",
    "        \n",
    "        loss = loss_criteria(scores,targets)\n",
    "        #print(scores)\n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        #adam\n",
    "        optimizer.step()\n",
    "            \n",
    "checkaccuracy(train_loader,modeldropout,\"Train\")\n",
    "checkaccuracy(test_loader,modeldropout,\"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions - 3\n",
    "\n",
    "As we see there is no much difference between the dropout [0.2] and [0.5] added to the layers . We can conclude that Neural network is not the apt solution to predict CIFAR images.\n",
    "\n",
    "We need to build a much higher evaluating model like CNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a CNN model\n",
    "\n",
    "we have built a CNN model of 6 convo 2d layers,  Relu , Maxpool2d. We have also used the functionality such as BatchNormal and Flatten to tune it.\n",
    "\n",
    "Lets check the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "QM8gnwuZp_ns"
   },
   "outputs": [],
   "source": [
    "class CnnModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CnnModel,self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
    "            nn.BatchNorm2d(128),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
    "            nn.BatchNorm2d(256),\n",
    "\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(256*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Got0.9612799882888794 with accuracy 96.128 Train\n",
      "Got0.8316999673843384 with accuracy 83.17 Test\n"
     ]
    }
   ],
   "source": [
    "modelCNN = CnnModel().to(device)\n",
    "#Loss and optimiser\n",
    "\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "betas=(0.9,0.995)\n",
    "weight_decay=5e-4\n",
    "optimizer = optim.Adam(modelCNN.parameters(), lr=learning_rate,betas=betas,weight_decay=weight_decay)\n",
    "#optimizer = optim.SGD(modelCNN.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "\n",
    "trainloader , testloader = reshapedataset()\n",
    "train_loader = DataLoader(trainloader , batch_size= batch_size, shuffle = False )\n",
    "test_loader = DataLoader(testloader , batch_size= batch_size, shuffle = False )\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #check for each each batch loader from a tuple\n",
    "    \n",
    "    for batch, (data, targets ) in enumerate(train_loader):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        #print(data.shape)\n",
    "        #reshape to single vector , correct shape\n",
    "        #trainloader= DataLoader(train_subset, batch_size=batch_size, shuffle=True) \n",
    "        #print(data.shape)\n",
    "        #data = data.reshape(data.shape[0],-1)\n",
    "        #print(data.shape)\n",
    "        scores = modelCNN(data)\n",
    "        \n",
    "        loss = loss_criteria(scores,targets)\n",
    "        #print(scores)\n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        #adam\n",
    "        optimizer.step()\n",
    "\n",
    "checkaccuracy(train_loader,modelCNN,\"Train\")\n",
    "checkaccuracy(test_loader,modelCNN,\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions -4\n",
    "\n",
    "As we see there is drastic improvement in the accuracy of the model by using CNN. but still there is 13 % error rate between the training and the testing set. Lets add dropouts and check the error rate between both. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnModel(nn.Module):\n",
    "    def __init__(self,dropout):\n",
    "        super(CnnModel,self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
    "            nn.BatchNorm2d(128),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
    "            nn.BatchNorm2d(256),\n",
    "\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(256*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "am9Yv-Xnp_ns",
    "outputId": "53763919-2535-42e5-9f1b-de43646a8b41",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Got0.9057599902153015 with accuracy 90.57600000000001 Train\n",
      "Got0.8037999868392944 with accuracy 80.38 Test\n"
     ]
    }
   ],
   "source": [
    "modelCNN = CnnModel(dropout = 0.2).to(device)\n",
    "#Loss and optimiser\n",
    "dropout = 0.2\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "betas=(0.9,0.995)\n",
    "weight_decay=5e-4\n",
    "optimizer = optim.Adam(modelCNN.parameters(), lr=learning_rate,betas=betas,weight_decay=weight_decay)\n",
    "#optimizer = optim.SGD(modelCNN.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "\n",
    "trainloader , testloader = reshapedataset()\n",
    "train_loader = DataLoader(trainloader , batch_size= batch_size, shuffle = False )\n",
    "test_loader = DataLoader(testloader , batch_size= batch_size, shuffle = False )\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #check for each each batch loader from a tuple\n",
    "    \n",
    "    for batch, (data, targets ) in enumerate(train_loader):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        #print(data.shape)\n",
    "        #reshape to single vector , correct shape\n",
    "        #trainloader= DataLoader(train_subset, batch_size=batch_size, shuffle=True) \n",
    "        #print(data.shape)\n",
    "        #data = data.reshape(data.shape[0],-1)\n",
    "        #print(data.shape)\n",
    "        scores = modelCNN(data)\n",
    "        \n",
    "        loss = loss_criteria(scores,targets)\n",
    "        #print(scores)\n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        #adam\n",
    "        optimizer.step()\n",
    "\n",
    "checkaccuracy(train_loader,modelCNN,\"Train\")\n",
    "checkaccuracy(test_loader,modelCNN,\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Got0.8759199976921082 with accuracy 87.592 Train\n",
      "Got0.796999990940094 with accuracy 79.7 Test\n"
     ]
    }
   ],
   "source": [
    "modelCNN = CnnModel(dropout = 0.3).to(device)\n",
    "#Loss and optimiser\n",
    "dropout = 0.5\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "betas=(0.9,0.995)\n",
    "weight_decay=5e-4\n",
    "optimizer = optim.Adam(modelCNN.parameters(), lr=learning_rate,betas=betas,weight_decay=weight_decay)\n",
    "#optimizer = optim.SGD(modelCNN.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "\n",
    "trainloader , testloader = reshapedataset()\n",
    "train_loader = DataLoader(trainloader , batch_size= batch_size, shuffle = False )\n",
    "test_loader = DataLoader(testloader , batch_size= batch_size, shuffle = False )\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #check for each each batch loader from a tuple\n",
    "    \n",
    "    for batch, (data, targets ) in enumerate(train_loader):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        #print(data.shape)\n",
    "        #reshape to single vector , correct shape\n",
    "        #trainloader= DataLoader(train_subset, batch_size=batch_size, shuffle=True) \n",
    "        #print(data.shape)\n",
    "        #data = data.reshape(data.shape[0],-1)\n",
    "        #print(data.shape)\n",
    "        scores = modelCNN(data)\n",
    "        \n",
    "        loss = loss_criteria(scores,targets)\n",
    "        #print(scores)\n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        #adam\n",
    "        optimizer.step()\n",
    "\n",
    "checkaccuracy(train_loader,modelCNN,\"Train\")\n",
    "checkaccuracy(test_loader,modelCNN,\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions \n",
    "\n",
    "As we clearly see we get the best accuracy of the model when the dropout is 0.2 and the error rate between the data is just 9-10%, we can consider this as our best working model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ZE8V29op_nv"
   },
   "outputs": [],
   "source": [
    "def plot_accuracies(history):\n",
    "    Validation_accuracies = [x['Accuracy'] for x in history]\n",
    "    Training_Accuracies = [x['train_accuracy'] for x in history]\n",
    "    plt.plot(Training_Accuracies, '-rx')\n",
    "    plt.plot(Validation_accuracies, '-bx')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.title('Accuracy vs. No. of epochs');\n",
    "plot_accuracies(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Task4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
