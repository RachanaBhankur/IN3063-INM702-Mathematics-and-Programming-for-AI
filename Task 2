
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f33d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import math as m\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fcca640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    #Implements the sigmoid activation in numpy#\n",
    "    \n",
    "    sO = 1/(1+np.exp(-Z))\n",
    "    \n",
    "    \n",
    "    return sO\n",
    "\n",
    "def relu(Z):\n",
    "    #Implement the RELU function#\n",
    "    \n",
    "    rO = np.maximum(0,Z)\n",
    "     \n",
    "    return rO\n",
    "\n",
    "def softmax(Z):\n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
    "    # softmax_memory is stored as it is used later on in backpropagation\n",
    "   \n",
    "    Z_exp = np.exp(Z - np.max(Z)) # np.exp(Z - np.max(Z))\n",
    "\n",
    "    Z_sum = np.sum(Z_exp,axis = 0, keepdims = True)\n",
    "    \n",
    "    smo = Z_exp/Z_sum \n",
    "   \n",
    "    \n",
    "    return smo\n",
    "\n",
    "def relu_backward(dA, rel):\n",
    "    #Implement the backward propagation for a single RELU unit.#\n",
    "    \n",
    "    Z = rel\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, sig):\n",
    "    #Implement the backward propagation for a single SIGMOID unit#\n",
    "    \n",
    "    \n",
    "    Z = sig\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "   \n",
    "    \n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b7368d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ParametersIntialising_Layers(dimLayer):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(dimLayer)            \n",
    "    for l in range(1, L):\n",
    "       \n",
    "        parameters['W' + str(l)] = np.random.randn(dimLayer[l],dimLayer[l-1])*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((dimLayer[l],1))\n",
    "     \n",
    "        \n",
    "       \n",
    "        \n",
    "    return parameters"
   ]
  },
  "source": [
    "(x_tr, y_tr), (x_test, y_tst) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_train = x_tr.reshape(60000,-1)[:50000].T\n",
    "x_val = x_tr.reshape(60000,-1)[50000:60000].T\n",
    "x_test = x_test.reshape(10000,-1).T\n",
    "\n",
    "\n",
    "y_trr = np.zeros((60000,10))\n",
    "y_trr[np.arange(y_tr.size),y_tr] = 1\n",
    "\n",
    "y_train = y_trr[:50000]\n",
    "y_val = y_trr[50000:60000]\n",
    "y_test = np.zeros((10000,10))\n",
    "y_test[np.arange(y_tst.size),y_tst] = 1\n",
    "y_train = y_train.T\n",
    "y_val = y_val.T\n",
    "y_test = y_test.T\n",
    "\n",
    "print(x_train.shape,x_val.shape,x_test.shape,y_train.shape,y_val.shape,y_test.shape)\n",
    "\n",
    "\n"
   ]
  },
